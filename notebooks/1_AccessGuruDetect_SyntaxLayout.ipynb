{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-bwLFBqcrt_V",
        "zV-XrXl9rynk",
        "T6BoCRU3r44i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  AccessGuru Detect Notebook\n",
        "Full Pipline:\n",
        "Full Pipline:\n",
        "1. **AccessGuruDetect SyntaxLayout**: Detect violations (Axe-Playwright) [Current Notebook]\n",
        "2. **AccessGuruDetect Semantic**: Detect violations (LLM)[Notebook Link](https://colab.research.google.com/drive/1A3GAA0LhK8gzLzPLw7sBO3Ng0mVHd4f4?usp=sharing)\n",
        "3. **AccessGuruCorrect**: Generate corrections using LLM prompting strategies. [Notebook Link](https://colab.research.google.com/drive/1zoW8fL6VLz1sE8BoHbfnIaaOrgMeNKC5?usp=drive_link)\n",
        "\n",
        "This notebook demonstrates a full pipeline for **AccessGuruDetect**: Detect violations (Axe-Playwright + LLM)\n",
        "We’ll walk through each step with explanations and runnable code."
      ],
      "metadata": {
        "id": "sn944O7krgrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. AccessGuruDetect\n",
        "We implemented the AccessGuruDetect using\n",
        "Axe-Playwright-1.51.0 for syntax and layout accessibility\n",
        "violations."
      ],
      "metadata": {
        "id": "NSR-QGrPrseD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Install Dependencies\n",
        "Use \"pip install\" to install the package"
      ],
      "metadata": {
        "id": "-bwLFBqcrt_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWp9sBXkoznZ"
      },
      "outputs": [],
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget\n",
        "! pip install selenium"
      ],
      "metadata": {
        "id": "t5ekZghxrv1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Imports & Setup"
      ],
      "metadata": {
        "id": "zV-XrXl9rynk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import wget\n",
        "import requests\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "jZq3Repory46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output directories\n",
        "output_dir = \"/content/html_pages_async\"\n",
        "screenshot_dir = \"/content/element_screenshots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(screenshot_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "b-dimC4Wr1WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required data(violation taxonomy, mapping dictionary) from AccessGuru Repo\n",
        "! wget 'https://raw.githubusercontent.com/NadeenAhmad/AccessGuruLLM/refs/heads/main/data/prompts_support/violation_taxonomy.csv'\n",
        "! wget 'https://raw.githubusercontent.com/NadeenAhmad/AccessGuruLLM/refs/heads/main/data/prompts_support/mapping_dict_file.json'\n",
        "! wget 'https://raw.githubusercontent.com/NadeenAhmad/AccessGuruLLM/refs/heads/main/data/prompts_support/violations_short_description.json'"
      ],
      "metadata": {
        "id": "Wy5qfgLnr2Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_dict_path = '/content/mapping_dict_file.json'\n",
        "with open(mapping_dict_path, 'r') as file:\n",
        "  mapping_dict = json.load(file)\n",
        "\n",
        "violation_description_path = '/content/violations_short_description.json'\n",
        "with open(violation_description_path, 'r') as file:\n",
        "  violation_description_dict = json.load(file)\n",
        "\n",
        "taxonomy_path = \"/content/violation_taxonomy.csv\"\n",
        "cat_data = pd.read_csv(taxonomy_path)\n"
      ],
      "metadata": {
        "id": "lPOKPnFfr3lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impactScore = {\n",
        "  \"critical\": 5,\n",
        "  \"serious\": 4,\n",
        "  \"moderate\": 3,\n",
        "  \"minor\": 2,\n",
        "  \"cosmetic\": 1,\n",
        "}\n",
        "\n",
        "impact_dict = {\n",
        "      'image-alt-not-descriptive': 'critical',\n",
        "      'video-captions-not-descriptive': 'critical',\n",
        "      'lang-mismatch': 'serious',\n",
        "      'missing-lang-tag': 'serious',\n",
        "      'link-text-mismatch': 'serious',\n",
        "      'button-label-mismatch': 'critical',\n",
        "      'form-label-mismatch': 'critical',\n",
        "      'ambiguous-heading': 'moderate',\n",
        "      'incorrect-semantic-tag': 'serious',\n",
        "      'landmark-structural-violation': 'serious',\n",
        "      'landmark-purpose-mismatch': 'serious',\n",
        "      'page-title-not-descriptive': 'serious',\n",
        "      'autocomplete-purpose-mismatch': 'serious',\n",
        "      'color-only-distinction': 'serious',\n",
        "      'illogical-focus-order': 'serious',\n",
        "      'label-name-mismatch': 'serious'\n",
        "       }"
      ],
      "metadata": {
        "id": "6NIOH0zUsPbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Utility Functions\n",
        "modules needed for the Detection:\n",
        "*   Download images,\n",
        "*   Check if given URL can be scraped\n",
        "*   save scraped HTML code,\n",
        "*   supplementary information extraction ."
      ],
      "metadata": {
        "id": "T6BoCRU3r44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def download_image(url, path):\n",
        "    try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(url) as resp:\n",
        "                if resp.status == 200:\n",
        "                    with open(path, 'wb') as f:\n",
        "                        f.write(await resp.read())\n",
        "                    print(f\"Image downloaded: {path}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"Failed to download image, status: {resp.status}\")\n",
        "                    return False\n",
        "    except Exception as e:\n",
        "        print(f\"Exception during image download: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "async def save_html(html, url):\n",
        "    parsed = urlparse(url)\n",
        "    netloc = parsed.netloc.replace(\".\", \"_\")\n",
        "    path = parsed.path.strip(\"/\") or \"home\"\n",
        "    path = \"\".join([c if c.isalnum() else \"_\" for c in path])\n",
        "    file_name = f\"{netloc}_{path}.html\"\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html)\n",
        "\n",
        "    return file_path\n",
        "\n",
        "async def url_check(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        try:\n",
        "            # Try navigating with a timeout\n",
        "            response = await page.goto(url, timeout=15000, wait_until=\"domcontentloaded\")\n",
        "\n",
        "            if not response:\n",
        "                print(f'No response for {url}. Please try another URL')\n",
        "                return \"not scraped\"\n",
        "\n",
        "            status = response.status\n",
        "            final_url = page.url\n",
        "\n",
        "            if status >= 400:\n",
        "                print(f\"Failed to load {url} (status {status}). Please try another URL\")\n",
        "                return \"not scraped\"\n",
        "\n",
        "            print(f\"Loaded {final_url} (status {status})\")\n",
        "            scrape_status = \"scraped\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}. Please try another URL\")\n",
        "            return \"not scraped\"\n",
        "        finally:\n",
        "            await browser.close()\n",
        "\n",
        "\n",
        "def get_full_list_html(web_html: str, affected_html: str) -> str | None:\n",
        "    soup = BeautifulSoup(web_html, \"html.parser\")\n",
        "\n",
        "    # Parse the affected HTML to extract the tag and attributes\n",
        "    affected_soup = BeautifulSoup(affected_html, \"html.parser\")\n",
        "    affected_element = affected_soup.find()\n",
        "\n",
        "    if not affected_element:\n",
        "        print(\"Could not parse affected HTML\")\n",
        "        return None\n",
        "\n",
        "    # Find matching element in full page HTML\n",
        "    matches = soup.find_all(affected_element.name, attrs=affected_element.attrs)\n",
        "\n",
        "    for match in matches:\n",
        "        # Return the outer HTML of the matching list\n",
        "        if match.name in ['ul', 'ol']:\n",
        "            return str(match)\n",
        "\n",
        "    print(\"No matching full list element found.\")\n",
        "    return None\n",
        "\n",
        "def find_matching_ul(soup, snippet_html):\n",
        "    snippet_soup = BeautifulSoup(snippet_html, 'html.parser')\n",
        "    snippet_ul = snippet_soup.find('ul')\n",
        "    if not snippet_ul:\n",
        "        return None\n",
        "\n",
        "    snippet_classes = set(snippet_ul.get('class', []))\n",
        "\n",
        "    for ul in soup.find_all('ul'):\n",
        "        ul_classes = set(ul.get('class', []))\n",
        "        if snippet_classes.issubset(ul_classes):\n",
        "            return str(ul)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_landmark_container_for_tag(soup, tag_name='main'):\n",
        "    tag = soup.find(lambda tag: tag.name == tag_name or tag.get('role', '').lower() == tag_name)\n",
        "    if not tag:\n",
        "        return None, f\"No <{tag_name}> tag or role='{tag_name}' found\"\n",
        "\n",
        "    landmark_roles = {'banner', 'complementary', 'main', 'contentinfo', 'navigation', 'region'}\n",
        "    current = tag.parent\n",
        "\n",
        "    while current:\n",
        "        role = current.get('role', '').lower()\n",
        "        if role in landmark_roles or current.name in landmark_roles:\n",
        "            return current, None\n",
        "        current = current.parent if hasattr(current, 'parent') else None\n",
        "\n",
        "    return tag, None\n",
        "\n",
        "\n",
        "def role_or_tag(role_value, tag_name):\n",
        "    return lambda tag: tag.name == tag_name or tag.attrs.get(\"role\") == role_value\n",
        "\n",
        "\n",
        "def extract_supplementary_info(row):\n",
        "    # Skip if supplementary_information already exists and is non-empty\n",
        "    if pd.notna(row.get(\"supplementary_information\")) and str(row.get(\"supplementary_information\")).strip():\n",
        "        return row[\"supplementary_information\"]\n",
        "\n",
        "    violation = row[\"violation_name\"]\n",
        "    html_file = row[\"html_file_name\"]\n",
        "\n",
        "    if not html_file.endswith(('.html', '.txt')):\n",
        "        html_file += '.html'\n",
        "\n",
        "    snippet = row[\"affected_html_elements\"]\n",
        "\n",
        "    try:\n",
        "        with open(html_file, 'r', encoding='utf-8') as f:\n",
        "            soup = BeautifulSoup(f, 'lxml')\n",
        "    except Exception as e:\n",
        "        return f\"HTML load error: {e}\"\n",
        "\n",
        "    # ---------- Violation-Specific Logic ----------\n",
        "\n",
        "    if \"color-contrast\" in violation or \"contrast-enhanced\" in violation:\n",
        "        return row[\"supplementary_information\"]\n",
        "\n",
        "\n",
        "    elif any(v in violation for v in [\"ambiguous-heading\", \"empty-heading\", \"heading-order\"]):\n",
        "        headings = soup.find_all(re.compile(r'^h[1-6]$'))\n",
        "        results = []\n",
        "\n",
        "        for heading in headings:\n",
        "            if not heading.get_text(strip=True):\n",
        "                next_elements = []\n",
        "                sibling = heading.find_next_sibling()\n",
        "                while sibling and len(next_elements) < 3:\n",
        "                    if sibling.name in [\"p\", \"ul\", \"ol\", \"div\", \"section\"]:\n",
        "                        next_elements.append(str(sibling))\n",
        "                    sibling = sibling.find_next_sibling()\n",
        "                results.append(f\"{str(heading)}\\n\\n\" + \"\\n\\n\".join(next_elements))\n",
        "\n",
        "        return \"\\n\\n---\\n\\n\".join(results) if results else \"\"\n",
        "\n",
        "    elif \"empty-table-header\" in violation:\n",
        "        headers = soup.find_all(\"th\")\n",
        "        results = []\n",
        "\n",
        "        for th in headers:\n",
        "            if not th.get_text(strip=True):\n",
        "                next_elements = []\n",
        "                sibling = th.find_next_sibling()\n",
        "                while sibling and len(next_elements) < 3:\n",
        "                    if sibling.name in [\"td\", \"th\", \"tr\"]:\n",
        "                        next_elements.append(str(sibling))\n",
        "                    sibling = sibling.find_next_sibling()\n",
        "                results.append(f\"{str(th)}\\n\\n\" + \"\\n\\n\".join(next_elements))\n",
        "\n",
        "        return \"\\n\\n---\\n\\n\".join(results) if results else \"\"\n",
        "\n",
        "    elif \"page-has-heading-one\" in violation:\n",
        "        title_html = str(soup.title) if soup.title and soup.title.string else \"\"\n",
        "        h1_tags = soup.find_all(\"h1\")\n",
        "        h1_html = \"\\n\\n\".join(str(h) for h in h1_tags[:3]) if h1_tags else \"\"\n",
        "        return f\"{title_html}\\n\\n---\\n\\n{h1_html}\"\n",
        "\n",
        "    elif \"page-title-not-descriptive\" in violation:\n",
        "        title_html = str(soup.title) if soup.title and soup.title.string else \"\"\n",
        "        headings = soup.find_all(re.compile(r\"^h[1-6]$\"))\n",
        "        heading_html = [str(h) for h in headings[:10]]\n",
        "        return f\"{title_html}\\n\\n---\\n\\n\" + \"\\n\\n\".join(heading_html) if heading_html else title_html\n",
        "\n",
        "    elif \"document-title\" in violation:\n",
        "        title_html = str(soup.title) if soup.title and soup.title.string and soup.title.string.strip() else \"\"\n",
        "        # title_html = str(soup.title) if soup.title and soup.title.string.strip() else \"\"\n",
        "        headings = soup.find_all(re.compile(r\"^h[1-6]$\"))\n",
        "        heading_html = [str(h) for h in headings[:10]]\n",
        "        return f\"{title_html}\\n\\n---\\n\\n\" + \"\\n\\n\".join(heading_html) if heading_html else title_html\n",
        "\n",
        "    elif any(v in violation for v in [\n",
        "        \"duplicate-id\", \"duplicate-id-aria\", \"duplicate-id-active\",\n",
        "        \"landmark-no-duplicate-contentinfo\", \"landmark-no-duplicate-main\",\n",
        "        \"landmark-no-duplicate-banner\", \"landmark-unique\"\n",
        "    ]):\n",
        "        report = []\n",
        "\n",
        "        # Duplicate ID check\n",
        "        if any(v in violation for v in [\"duplicate-id\", \"duplicate-id-aria\", \"duplicate-id-active\"]):\n",
        "            id_map = {}\n",
        "            for tag in soup.find_all(attrs={\"id\": True}):\n",
        "                id_map.setdefault(tag[\"id\"], []).append(tag)\n",
        "\n",
        "            duplicates = {k: v for k, v in id_map.items() if len(v) > 1}\n",
        "            for dup_id, elements in list(duplicates.items())[:5]:\n",
        "                report.append(f\"ID '{dup_id}' is used {len(elements)} times:\")\n",
        "                for el in elements[:3]:\n",
        "                    snippet = str(el)\n",
        "                    report.append(snippet if len(snippet) <= 500 else snippet[:500] + \"...\")\n",
        "\n",
        "        # Duplicate landmarks\n",
        "        if \"landmark-no-duplicate-contentinfo\" in violation:\n",
        "            contentinfos = soup.find_all(role_or_tag(\"contentinfo\", \"footer\"))\n",
        "            if len(contentinfos) > 1:\n",
        "                report.append(f\"{len(contentinfos)} <footer> or role='contentinfo' elements found:\\n\" +\n",
        "                              \"\\n---\\n\".join(str(tag) for tag in contentinfos))\n",
        "\n",
        "        if \"landmark-no-duplicate-main\" in violation:\n",
        "            mains = soup.find_all(role_or_tag(\"main\", \"main\"))\n",
        "            if len(mains) > 1:\n",
        "                report.append(f\"{len(mains)} <main> or role='main' elements found:\\n\" +\n",
        "                              \"\\n---\\n\".join(str(tag) for tag in mains))\n",
        "\n",
        "        if \"landmark-no-duplicate-banner\" in violation:\n",
        "            banners = soup.find_all(role_or_tag(\"banner\", \"header\"))\n",
        "            if len(banners) > 1:\n",
        "                report.append(f\"{len(banners)} <header> or role='banner' elements found:\\n\" +\n",
        "                              \"\\n---\\n\".join(str(tag) for tag in banners))\n",
        "\n",
        "        if \"landmark-unique\" in violation:\n",
        "            roles = [\"main\", \"banner\", \"contentinfo\", \"navigation\", \"search\", \"complementary\", \"form\"]\n",
        "            for role in roles:\n",
        "                tags = soup.find_all(attrs={\"role\": role})\n",
        "                if len(tags) > 1:\n",
        "                    report.append(f\"Role '{role}' found {len(tags)} times:\\n\" +\n",
        "                                  \"\\n---\\n\".join(str(tag) for tag in tags))\n",
        "\n",
        "        return \"\\n\\n\".join(report) if report else \"\"\n",
        "\n",
        "    elif violation in [\n",
        "        \"landmark-main-is-top-level\", \"landmark-banner-is-top-level\", \"landmark-complementary-is-top-level\"\n",
        "    ]:\n",
        "        tag_map = {\n",
        "            \"landmark-main-is-top-level\": \"main\",\n",
        "            \"landmark-banner-is-top-level\": \"banner\",\n",
        "            \"landmark-complementary-is-top-level\": \"complementary\"\n",
        "        }\n",
        "        tag_role = tag_map.get(violation, \"main\")\n",
        "        container, error = get_landmark_container_for_tag(soup, tag_role)\n",
        "        return str(container) if container else \"\"\n",
        "\n",
        "    elif any(v in violation for v in [\n",
        "        \"lang-mismatch\", \"missing-lang-tag\", \"html-lang-valid\",\n",
        "        \"html-xml-lang-mismatch\", \"valid-lang\", \"html-has-lang\"\n",
        "    ]):\n",
        "        title = soup.title.string.strip() if soup.title and soup.title.string else \"No <title> tag or title is empty\"\n",
        "        headings = soup.find_all(re.compile(r'^h[1-6]$'))\n",
        "        heading_texts = [f\"{h.name.upper()}: {h.get_text(strip=True)}\" for h in headings if h.get_text(strip=True)]\n",
        "        return f\"Title: {title} | Headings: {' | '.join(heading_texts[:10])}\" if heading_texts else f\"Title: {title}\"\n",
        "\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "5J3i4Pt2r5Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. Web Accessibility Detection\n",
        "Runs Playwright + Axe-core to find accessibility violations. <br>\n",
        "Scraping the html code is integrated in the same code\n"
      ],
      "metadata": {
        "id": "gXv9_Eusr_pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def check_accessibility(url):\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "\n",
        "            try:\n",
        "                # Try navigating with a timeout\n",
        "                response = await page.goto(url, timeout=15000, wait_until=\"domcontentloaded\")\n",
        "\n",
        "                if not response:\n",
        "                    # return f'No response for {url}. Please try another URL'\n",
        "                    scrape_status = \"not scraped\"\n",
        "                    return None, page, browser, None, None,scrape_status\n",
        "\n",
        "                status = response.status\n",
        "                final_url = page.url\n",
        "\n",
        "                if status >= 400:\n",
        "                    # return (f\"Failed to load {url} (status {status}). Please try another URL\")\n",
        "                    scrape_status = \"not scraped\"\n",
        "                    return None, page, browser, None, None,scrape_status\n",
        "\n",
        "                print(f\"Loaded {final_url} (status {status})\")\n",
        "                scrape_status = \"scraped\"\n",
        "            except Exception as e:\n",
        "                # return(f\"Error scraping {url}. Please try another URL\")\n",
        "                scrape_status = \"not scraped\"\n",
        "                return None, page, browser, None, None,scrape_status\n",
        "\n",
        "            await page.goto(url)\n",
        "            html = await page.content()\n",
        "            html_file_name = await save_html(html, url)  # Save the HTML\n",
        "\n",
        "            await page.add_script_tag(url=\"https://cdn.jsdelivr.net/npm/axe-core@4.4.1/axe.min.js\")\n",
        "\n",
        "            results = await page.evaluate(\"\"\"\n",
        "            () => axe.run(document, {\n",
        "                runOnly: {\n",
        "                    type: 'tag',\n",
        "                    values: [\n",
        "                        'ACT', 'EN-301-549', 'EN-9.1.1.1', 'EN-9.1.2.2', 'EN-9.1.3.1', 'EN-9.1.3.5',\n",
        "                        'EN-9.1.4.1', 'EN-9.1.4.12', 'EN-9.1.4.2', 'EN-9.1.4.3', 'EN-9.1.4.4',\n",
        "                        'EN-9.2.1.1', 'EN-9.2.1.3', 'EN-9.2.2.1', 'EN-9.2.2.2', 'EN-9.2.4.1',\n",
        "                        'EN-9.2.4.2', 'EN-9.2.4.4', 'EN-9.3.1.1', 'EN-9.3.1.2', 'EN-9.3.3.2',\n",
        "                        'EN-9.4.1.2', 'TT11.a', 'TT11.b', 'TT12.a', 'TT12.d', 'TT13.a', 'TT13.c',\n",
        "                        'TT14.b', 'TT17.a', 'TT2.a', 'TT2.b', 'TT4.a', 'TT5.c', 'TT6.a', 'TT7.a', 'TT7.b',\n",
        "                        'TT8.a', 'TT9.a', 'TTv5', 'best-practice', 'cat.aria', 'cat.color', 'cat.forms',\n",
        "                        'cat.keyboard', 'cat.language', 'cat.name-role-value', 'cat.parsing',\n",
        "                        'cat.semantics', 'cat.sensory-and-visual-cues', 'cat.structure',\n",
        "                        'cat.tables', 'cat.text-alternatives', 'cat.time-and-media', 'review-item',\n",
        "                        'section508', 'section508.22.a', 'section508.22.f', 'section508.22.g',\n",
        "                        'section508.22.i', 'section508.22.j', 'section508.22.n', 'section508.22.o',\n",
        "                        'wcag111', 'wcag122', 'wcag131', 'wcag135', 'wcag141', 'wcag1412', 'wcag142',\n",
        "                        'wcag143', 'wcag144', 'wcag146', 'wcag211', 'wcag213', 'wcag21aa', 'wcag221',\n",
        "                        'wcag222', 'wcag224', 'wcag22aa', 'wcag241', 'wcag242', 'wcag244', 'wcag249',\n",
        "                        'wcag258', 'wcag2a', 'wcag2aa', 'wcag2aaa', 'wcag311', 'wcag312', 'wcag325',\n",
        "                        'wcag332', 'wcag412'\n",
        "                    ]\n",
        "                }\n",
        "            })\n",
        "            \"\"\")\n",
        "\n",
        "            return results, page, browser,html_file_name,html,scrape_status\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "ntk-MctCr8mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST check_accessibility()\n",
        "# url = \"https://www.futurity.org\"\n",
        "# results, page, browser, html_file_name, web_html,scrape_status = await check_accessibility(url)\n",
        "# print(\"HTML FILE PATH:\",html_file_name)\n",
        "# print(results)\n",
        "\n",
        "# violations = results['violations']\n",
        "# print(f'Violations Deteced for the URL: {url}')\n",
        "# print(\"*\"*100)\n",
        "# print(violations)"
      ],
      "metadata": {
        "id": "A0Xph6y5sIl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5. AccessGuru Detect"
      ],
      "metadata": {
        "id": "MJe2dqpGsGcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5.a AccessGuru Detect for syntactic and layout accessibility violations\n",
        "For the given input URL,\n",
        "run accessibility detection URLs → violations → supplementary info → structured\n",
        "\n",
        "* run check_accessibility()\n",
        "    * Which returns list of violations,\n",
        "    * Extract description, impact, help_url, html_code from the each violations.\n",
        "    * Extract Supplementary Information from the html\n",
        "    * Return {\n",
        "                        'web_URL': 'The URL of the webpage where the violation was found'\n",
        "                        'scrape_status':'Status indicating whether the webpage was successfully scraped or not'\n",
        "                        'violation_count': 'Total number of accessibility violations found on the page,\n",
        "                        'violation_name': 'Name of the violated accessibility',\n",
        "                        'violation_score': 'Score representing the impact of the violation',\n",
        "                        'violation_description': 'Short description of the violation type',\n",
        "                        'violation_description_url': 'URL linking to a detailed explanation of the violation',\n",
        "                        'affected_html_elements': 'Snippet of HTML showing the elements affected by the violation',\n",
        "                        'html_file_name': 'Filename of the saved HTML file associated with the scraped page',\n",
        "                        'supplementary_information': 'Extra Snippet of HTML relevant to the violation needed for LLMs to understand the affected_html_elements'\n",
        "                    }"
      ],
      "metadata": {
        "id": "Og_0ORnlsPsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "impactScore = {\n",
        "  \"critical\": 5,\n",
        "  \"serious\": 4,\n",
        "  \"moderate\": 3,\n",
        "  \"minor\": 2,\n",
        "  \"cosmetic\": 1,\n",
        "}\n",
        "\n",
        "scoreToImpact = {v: k for k, v in impactScore.items()}\n",
        "\n",
        "async def main(url, index, impactScore):\n",
        "    violation_dict_list = []\n",
        "\n",
        "    url_to_test = url\n",
        "    print(f\"URL to test: {url_to_test}\")\n",
        "    results, page, browser, html_file_name, web_html,scrape_status = await check_accessibility(url_to_test)\n",
        "\n",
        "    if results:\n",
        "        violations = results['violations']\n",
        "        if violations:\n",
        "            print(f\"Number of accessibility violations: {len(violations)}\")\n",
        "\n",
        "            try:\n",
        "                for violation in violations:\n",
        "                    supplementary_information_parts = []  # <-- use list to accumulate\n",
        "                    violation_id = violation['id']\n",
        "                    # print(\"======violation_id:\", violation_id)\n",
        "\n",
        "                    description = violation['description']\n",
        "                    impact = violation['impact']\n",
        "                    help_url = violation['helpUrl']\n",
        "                    html_code = \", \".join([node['html'] for node in violation['nodes']])\n",
        "\n",
        "                    # Supplementary Information: Color Violations\n",
        "                    if violation_id in ['color-only-distinction', 'color-contrast-enhanced', 'color-contrast']:\n",
        "                        try:\n",
        "                            data_value = violation['nodes'][0]['any'][0]['data']\n",
        "                            if isinstance(data_value, dict) and 'fgColor' in data_value:\n",
        "                                supplementary_information_parts.append(str(data_value))\n",
        "                        except (IndexError, KeyError):\n",
        "                            pass\n",
        "\n",
        "                    screenshot_paths = []\n",
        "\n",
        "                    # Supplementary Information: Image violations\n",
        "                    if violation_id in [\n",
        "                        \"image-alt\", \"input-image-alt\", \"image-alt-not-descriptive\",\n",
        "                        \"image-redundant-alt\", \"area-alt\", \"frame-title\", \"frame-title-unique\",\n",
        "                        \"object-alt\", \"role-img-alt\", \"svg-img-alt\", \"button-name\", \"input-button-name\"\n",
        "                    ]:\n",
        "                        for i, node in enumerate(violation['nodes']):\n",
        "                            html = node.get('html', '')\n",
        "                            img_src_match = re.search(r'<img[^>]+src=[\"\\']([^\"\\']+)[\"\\']', html)\n",
        "                            if img_src_match:\n",
        "                                img_url = img_src_match.group(1)\n",
        "                                parsed = urlparse(url)\n",
        "                                domain = parsed.netloc.replace('.', '_')\n",
        "                                filename = f\"{domain}_{violation_id}_{i}.png\"\n",
        "                                filepath = os.path.join(screenshot_dir, filename)\n",
        "\n",
        "                                success = await download_image(img_url, filepath)\n",
        "                                if success:\n",
        "                                    screenshot_paths.append(filepath)\n",
        "                            else:\n",
        "                                print(f\"No img src found in node HTML for violation {violation_id}, node index {i}\")\n",
        "\n",
        "                    if screenshot_paths:\n",
        "                        supplementary_information_parts.append(\", \".join(screenshot_paths))\n",
        "\n",
        "                    # Supplementary Information: List\n",
        "                    list_html_snippets = []\n",
        "                    if violation_id == \"list\":\n",
        "                        for i, node in enumerate(violation['nodes']):\n",
        "                            affected_html = node.get('html', '')\n",
        "                            full_list_html = get_full_list_html(web_html, affected_html)\n",
        "                            if full_list_html:\n",
        "                                list_html_snippets.append(full_list_html)\n",
        "\n",
        "                    if list_html_snippets:\n",
        "                        supplementary_information_parts.append(\"\\n\\n---\\n\\n\".join(list_html_snippets))\n",
        "\n",
        "                    # Supplementary Information: link-name\n",
        "                    link_info_list = []\n",
        "                    if violation_id == \"link-name\":\n",
        "                        for node in violation[\"nodes\"]:\n",
        "                            affected_html = node.get(\"html\", \"\")\n",
        "                            print(affected_html)\n",
        "\n",
        "                            href_match = re.search(r'href=[\"\\']([^\"\\']+)[\"\\']', affected_html)\n",
        "                            target_match = re.search(r'target=[\"\\']([^\"\\']+)[\"\\']', affected_html)\n",
        "\n",
        "                            href = href_match.group(1) if href_match else None\n",
        "                            explicit_target = target_match.group(1).lower() if target_match else None\n",
        "\n",
        "                            if not href or not href.startswith(\"http\"):\n",
        "                                continue\n",
        "\n",
        "                            try:\n",
        "                                if explicit_target == \"_blank\":\n",
        "                                    try:\n",
        "                                        async with async_playwright() as p:\n",
        "                                            browser1 = await p.chromium.launch()\n",
        "                                            page = await browser1.new_page()\n",
        "                                            await page.goto(href, timeout=15000)\n",
        "                                            html = await page.content()\n",
        "                                            soup = BeautifulSoup(html, 'html.parser')\n",
        "                                            title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "                                            link_info_list.append(f\"The title of the target {href} link page: {title}\")\n",
        "                                            await browser1.close()\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Error processing link '{href}': {e}\")\n",
        "\n",
        "                                elif explicit_target == \"_self\":\n",
        "                                    async with async_playwright() as p:\n",
        "                                        browser1 = await p.chromium.launch()\n",
        "                                        page1 = await browser1.new_page()\n",
        "                                        await page1.goto(href, timeout=15000)\n",
        "\n",
        "                                        page_title = await page1.title()\n",
        "                                        if not page_title:\n",
        "                                            html = await page1.content()\n",
        "                                            soup = BeautifulSoup(html, 'html.parser')\n",
        "                                            page_title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "\n",
        "                                        link_info_list.append(f\"The title of the target {href} link page: {page_title}\")\n",
        "                                        await browser1.close()\n",
        "\n",
        "                                else:\n",
        "                                    try:\n",
        "                                        async with async_playwright() as p:\n",
        "                                            browser1 = await p.chromium.launch()\n",
        "                                            page = await browser1.new_page()\n",
        "                                            await page.goto(href, timeout=15000)\n",
        "                                            html = await page.content()\n",
        "                                            soup = BeautifulSoup(html, 'html.parser')\n",
        "                                            title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "                                            link_info_list.append(f\"The title of the target {href} link page: {title}\")\n",
        "                                            await browser1.close()\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Error processing link '{href}': {e}\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error processing link '{href}': {e}\")\n",
        "\n",
        "                    if link_info_list:\n",
        "                        supplementary_information_parts.append(\"\\n\\n\".join(link_info_list))\n",
        "\n",
        "                    # Final supplementary info\n",
        "                    supplementary_information = \"\\n\\n\".join(supplementary_information_parts)\n",
        "\n",
        "                    index += 1\n",
        "                    violation_dict = {\n",
        "                        'web_URL': url,\n",
        "                        'scrape_status':scrape_status,\n",
        "                        'violation_count': len(violations),\n",
        "                        'violation_name': violation_id,\n",
        "                        'violation_score': impactScore.get(impact, \"Unknown\"),\n",
        "                        'violation_description': description,\n",
        "                        'violation_description_url': help_url,\n",
        "                        'affected_html_elements': html_code,\n",
        "                        'html_file_name': html_file_name,\n",
        "                        'supplementary_information': supplementary_information\n",
        "                    }\n",
        "                    violation_dict_list.append(violation_dict)\n",
        "\n",
        "                return violation_dict_list\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error in violation processing: {e}\")\n",
        "            finally:\n",
        "                print(\"Closing browser now...\")\n",
        "                await browser.close()\n",
        "        else:\n",
        "            print(\"No accessibility violations found.\")\n",
        "            await browser.close()\n",
        "            return [{\n",
        "                'web_URL': url,\n",
        "                'scrape_status': \"not scraped\",\n",
        "                'violation_count': 0,\n",
        "                'violation_name': None,\n",
        "                'violation_score': 0,\n",
        "                'violation_description': \"No violations found\",\n",
        "                'violation_description_url': None,\n",
        "                'affected_html_elements': None,\n",
        "                'html_file_name': None,\n",
        "                'supplementary_information': None\n",
        "            }]\n",
        "\n",
        "    else:\n",
        "        print(\"No results returned or an error occurred.\")\n",
        "        if browser:\n",
        "            await browser.close()\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "4DVBVhrUsCMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6. Example Run\n"
      ],
      "metadata": {
        "id": "J9YAjqs0sWb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6.a. Syntax and Layout Example Run\n",
        "The input should have the following values for each keys:\n",
        "*   **web_URL_id** : Unique identifier for the URL\n",
        "*   **domain_category** : The domain of the website's subject area (Domains: Educational Platforms, Government and Public Services, News and Media, E-commerce, Streaming Platforms, Health and Wellness, Technology, Science and Research )\n",
        "*   **web_URL** : The URL of the webpage where the violation was found\n",
        "\n",
        "**Input dictionary example**:\n",
        "```\n",
        "{\n",
        "  'web_URL_id':1, 'webURL':'https://www.ki.uni-stuttgart.de/', 'domain_category': 'Educational Platforms'\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "mrZjpS6_sYSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error URL test: URL that can't be scrapped\n",
        "# input_dict = {'web_URL_id':1, 'webURL':'google.com', 'domain_category': 'Educational Platforms'}\n",
        "\n",
        "# test input_dict\n",
        "input_dict = [\n",
        "              {'web_URL_id':2, 'web_URL':\"https://www.w3.org/WAI/content-assets/wcag-act-rules/testcases/c4a8a4/2c1397032aad720fe43dee2be0d326be56957320.html\",'domain_category': 'Educational Platforms','screenshot_path':\"/content/43.png\"},\n",
        "              ]\n",
        "\n",
        "url_df = pd.DataFrame(input_dict)\n",
        "url_df.head()\n",
        "urls = list(url_df[\"web_URL\"].values)\n",
        "\n",
        "output = pd.DataFrame()\n",
        "for url in urls:\n",
        "    scrape_status,html,html_file_name =  await url_check_AndHtml(url)\n",
        "    if scrape_status == \"not scraped\":\n",
        "       break\n",
        "    else:\n",
        "      url_df[\"scrape_status\"] = scrape_status\n",
        "      url_df[\"html_file_name\"] = html_file_name\n",
        "\n",
        "      screenshot_path = url_df[url_df[\"web_URL\"]==url][\"screenshot_path\"][0]\n",
        "      screenshot_data_url = encode_image_to_data_url(screenshot_path)\n",
        "\n",
        "      domain_category = url_df[url_df[\"web_URL\"]==url][\"domain_category\"][0]\n",
        "      prompt_text = generate_semantic_prompt(domain_category,url,taxonomy,html)\n",
        "      llm_response = generate_response(prompt_text,screenshot_data_url)\n",
        "\n",
        "      violations,violation_count = post_process_response(url,url_df,llm_response,html)\n",
        "      url_df[\"violation_count\"] = violation_count\n",
        "      for each_violation in violations:\n",
        "          df_dictionary = pd.DataFrame([each_violation])\n",
        "          output = pd.concat([output, df_dictionary], ignore_index=True)\n",
        "\n",
        "      if len(output)>0:\n",
        "          violation_df = pd.merge(output, url_df, on=\"web_URL\")\n",
        "          violation_df[\"wcag_reference\"] = violation_df[\"violation_name\"].map(mapping_dict)\n",
        "          # violation_df[\"supplementary_information\"]  = \"\"\n",
        "          violation_df[\"violation_category\"]  = \"Semantic\"\n",
        "          violation_df = violation_df[violation_df['violation_count'] != 0]\n",
        "      else:\n",
        "          print(\"===No Violations==\")\n",
        "          web_URL_id = url_df[url_df[\"web_URL\"]==url][\"web_URL_id\"][0]\n",
        "          new_id = str(web_URL_id)+\"_1\"\n",
        "          violations.append({\n",
        "            \"id\":new_id,\n",
        "            \"web_URL\":url,\n",
        "            \"affected_html_elements\": \"\",\n",
        "            \"violation_name\": \"\",\n",
        "            \"violation_description\":\"\",\n",
        "            \"violation_description_url\": \"\",\n",
        "            \"violation_impact\":\"\",\n",
        "            \"violation_score\":\"\"\n",
        "        })\n",
        "\n",
        "\n",
        "violation_df.head()"
      ],
      "metadata": {
        "id": "b7dbw8u1sSX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save & Export Results\n",
        "violation_df.to_csv(\"DetectedViolationData.csv\",index=False)"
      ],
      "metadata": {
        "id": "rK3AAbxUsbUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "774HvaE5wUti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}